{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import math\n",
    "from scipy.io import wavfile\n",
    "\n",
    "def precompute_min_timeseries_len_librosa(list_of_audiofiles, hop_length, sr):\n",
    "    timeseries_length_list = []\n",
    "    for file in list_of_audiofiles:\n",
    "        #print(\"Loading \" + str(file))\n",
    "        y, sr = librosa.load(file, sr=sr)\n",
    "        timeseries_length_list.append(math.ceil(len(y) / hop_length))\n",
    "    return min(timeseries_length_list)\n",
    "\n",
    "def precompute_min_timeseries_len_wavfile(list_of_audiofiles, hop_length):\n",
    "    timeseries_length_list = []\n",
    "    for file in list_of_audiofiles:\n",
    "        #print(\"Loading \" + str(file))\n",
    "        sr, y = wavfile.read(file)\n",
    "        timeseries_length_list.append(math.ceil(len(y) / hop_length))\n",
    "    return min(timeseries_length_list)\n",
    "\n",
    "def precompute_min_timeseries_len_from_data(array_of_data, hop_length):\n",
    "    timeseries_length_list = []\n",
    "    for data in array_of_data:\n",
    "        timeseries_length_list.append(math.ceil(len(data) / hop_length))\n",
    "    return min(timeseries_length_list)\n",
    "\n",
    "def extract_audio_features_librosa(list_of_audiofiles, sound_class, hop_length, sr):\n",
    "    list_of_data = []\n",
    "    for i, file in enumerate(list_of_audiofiles):\n",
    "        y, sr = librosa.load(file, sr=sr)\n",
    "        list_of_data.append(y)\n",
    "    return extract_audio_features_from_data(list_of_data, sound_class, hop_length, sr)\n",
    "\n",
    "def extract_audio_features_wavfile(list_of_audiofiles, sound_class, hop_length):\n",
    "    list_of_data = []\n",
    "    for i, file in enumerate(list_of_audiofiles):\n",
    "        print(file)\n",
    "        sr, y = wavfile.read(file)\n",
    "        list_of_data.append(y)\n",
    "    return extract_audio_features_from_data(list_of_data, sound_class, hop_length, sr)\n",
    "\n",
    "def extract_audio_features_from_data(list_of_data, sound_class, hop_length, sr):\n",
    "    n_mfcc = 13\n",
    "    whole_data = None\n",
    "    \n",
    "    for i, y in enumerate(list_of_data):\n",
    "        y = y.astype(np.float32)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=n_mfcc)\n",
    "        spectral_center = librosa.feature.spectral_centroid(y=y, sr=sr, hop_length=hop_length)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length)\n",
    "        spectral_contrast = librosa.feature.spectral_contrast(y=y, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "        timeseries_length = mfcc.T.shape[0] # precompute_min_timeseries_len_from_data([y], hop_length) + 1\n",
    "        data = np.zeros((timeseries_length, 33), dtype=np.float64)\n",
    "        #print('data shape: ', data.shape)\n",
    "        data[:, 0:13] = mfcc.T\n",
    "        data[:, 13:14] = spectral_center.T\n",
    "        data[:, 14:26] = chroma.T\n",
    "        data[:, 26:33] = spectral_contrast.T\n",
    "        \n",
    "        #print('timeseries_length: ', timeseries_length)\n",
    "        #print('sound_data: ', y[:20])\n",
    "        #print('shape: ', data.shape)\n",
    "        #print('before norm: ', data[:20, :])\n",
    "        data = data / np.linalg.norm(data)\n",
    "        #print('after norm: ', data[:20, :])\n",
    "        \n",
    "        if whole_data is None:\n",
    "            whole_data = data\n",
    "        else:\n",
    "            whole_data = np.vstack((whole_data, data))\n",
    "        \n",
    "    return whole_data, np.full((whole_data.shape[0], 1), sound_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(data):\n",
    "        familiar_count = float(len(data[data == 1]))\n",
    "        print('familiar_count', familiar_count)\n",
    "        all_data_count = float(len(data))\n",
    "        print('all_data_count', all_data_count)\n",
    "        familiar_percent = familiar_count / all_data_count\n",
    "        print('familiar_percent', familiar_percent)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_length_list = []\n",
    "hop_length = 512\n",
    "for sound_class, sound_paths in class_to_file_paths.items():\n",
    "    timeseries_length_list.append(precompute_min_timeseries_len_wavfile(sound_paths, hop_length))\n",
    "print(timeseries_length_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "hop_length = 512\n",
    "X_train_one_svm = None\n",
    "Y_train_one_svm = None\n",
    "\n",
    "for sound_class, sound_paths in class_to_file_paths.items():\n",
    "    data, target = extract_audio_features_wavfile(sound_paths, int(sound_class), hop_length)\n",
    "    if X_train_one_svm is None and Y_train_one_svm is None:\n",
    "        X_train_one_svm = data\n",
    "        Y_train_one_svm = target\n",
    "    else:\n",
    "        X_train_one_svm = np.vstack((X_train_one_svm, data))\n",
    "        Y_train_one_svm = np.vstack((Y_train_one_svm, target))\n",
    "print(X_train_one_svm.shape, Y_train_one_svm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.OneClassSVM(nu=0.1, coef0=0.09, kernel=\"poly\", gamma=0.1)\n",
    "clf.fit(X_train_one_svm)\n",
    "print_metrics(clf.predict(X_train_one_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(clf, open('one_class_model_for_5_classes.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_files_dir='fake-sounds'\n",
    "fake_files_paths = get_sound_files_paths(fake_files_dir)\n",
    "fake_data = read_full_data_from_drive(fake_files_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sound_class, sound_paths in fake_data.items():\n",
    "    for sound_path in sound_paths:\n",
    "        data, target = extract_audio_features_wavfile([sound_path], int(sound_class), hop_length)\n",
    "        print_metrics(clf.predict(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sound_class, sound_paths in class_to_file_paths.items():\n",
    "    for sound_path in sound_paths:\n",
    "        data, target = extract_audio_features_wavfile([sound_path], int(sound_class), hop_length)\n",
    "        print_metrics(clf.predict(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sound_files_dir='sounds-with-classes'\n",
    "sound_files_dir='sounds-with-classes-without-noise'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sound_files_paths(sound_files_dir):\n",
    "    from os import walk\n",
    "    sound_files_paths = []\n",
    "    for (dirpath, dirnames, filenames) in walk(sound_files_dir):\n",
    "        if len(filenames) > 0:\n",
    "            for filename in filenames:\n",
    "                if len(filename) > 0 and filename[0] != '.':\n",
    "                    sound_files_paths.append('/'.join([dirpath, filename]))\n",
    "    return sound_files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_files_paths = get_sound_files_paths(sound_files_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_full_data_from_drive(data_path):\n",
    "    class_to_file_paths = {}\n",
    "    for sound_file_path in data_path:\n",
    "        file_class = sound_file_path.split('/')[-2]\n",
    "        if file_class in class_to_file_paths:\n",
    "            class_to_file_paths[file_class].append(sound_file_path)\n",
    "        else:\n",
    "            class_to_file_paths[file_class] = [sound_file_path]\n",
    "    return class_to_file_paths\n",
    "\n",
    "def read_data_from_drive(sound_file_path):\n",
    "    from scipy.io import wavfile\n",
    "    fs, data = wavfile.read(sound_file_path)\n",
    "    #print(sound_file_path)\n",
    "    #print(data[:20])\n",
    "    #print(data.dtype)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_file_paths = read_full_data_from_drive(sound_files_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_to_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(read_data_from_drive(class_to_file_paths['4'][0]), rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_to_train_and_valid(data, valid_percent=20.0, test_percent=10.0):\n",
    "    import math\n",
    "    import random\n",
    "    train_data = {}\n",
    "    valid_data = {}\n",
    "    for class_id, sound_files_paths in data.items():\n",
    "        valid_samples_in_class = math.floor(len(sound_files_paths) * (valid_percent / 100))\n",
    "        train_samples_in_class = len(sound_files_paths) - valid_samples_in_class\n",
    "        valid_samples_ids = random.sample(range(len(sound_files_paths)), valid_samples_in_class)\n",
    "        for i in range(len(sound_files_paths)):\n",
    "            if i in valid_samples_ids:\n",
    "                valid_data[sound_files_paths[i]] = class_id\n",
    "            else:\n",
    "                train_data[sound_files_paths[i]] = class_id\n",
    "    return (train_data, valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = split_data_to_train_and_valid(class_to_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram_of_file(samples, sample_rate=44100):\n",
    "    from scipy import signal\n",
    "    frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "    return spectrogram.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_items_count(data):\n",
    "    items_count = 0\n",
    "    for file_path in data:\n",
    "        items_count += len(read_data_from_drive(file_path))\n",
    "    return items_count\n",
    "\n",
    "def get_spectrogram_count(data):\n",
    "    items_count = 0\n",
    "    for file_path in data:\n",
    "        items_count += get_spectrogram_of_file(read_data_from_drive(file_path)).shape[0]\n",
    "    return items_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_items_count = get_spectrogram_count(train_data)\n",
    "valid_data_items_count = get_spectrogram_count(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data)\n",
    "print(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(class_to_file_paths):\n",
    "    vocabulary = {}\n",
    "    vocab_index = 0\n",
    "    for file_class, file_paths in class_to_file_paths.items():\n",
    "        for file_path in file_paths:\n",
    "            file_data = read_data_from_drive(file_path)\n",
    "            for file_item in file_data:\n",
    "                if file_item not in vocabulary:\n",
    "                    vocabulary[file_item] = vocab_index\n",
    "                    vocab_index += 1\n",
    "    reversed_vocabulary = dict(zip(vocabulary.values(), vocabulary.keys()))\n",
    "    return (vocabulary, reversed_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary, reversed_vocabulary = build_vocabulary(class_to_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "class StepanBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, num_classes, vocabulary, skip_steps=1, freq=129):\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.skip_steps = skip_steps\n",
    "        self.vocabulary = vocabulary\n",
    "        self.freq = freq\n",
    "        \n",
    "        self.current_data = None\n",
    "        self.current_file_idx = 0\n",
    "        self.current_pos_in_file = 0\n",
    "        \n",
    "        self.current_spectrogram = None\n",
    "        self.current_pos_in_spectrogram = 0\n",
    "\n",
    "        self.file_paths = []\n",
    "        self.file_classes = []\n",
    "        for file_path, file_class in data.items():\n",
    "            self.file_paths.append(file_path)\n",
    "            self.file_classes.append(file_class)\n",
    "        self.__change_file()\n",
    "    \n",
    "    def __map_data_to_vocabulary(self, data):\n",
    "        return [self.vocabulary[d] for d in data if d in self.vocabulary]\n",
    "    \n",
    "    def __change_file(self):\n",
    "        while True:\n",
    "            try:\n",
    "                if self.current_file_idx+1 >= len(self.file_paths):\n",
    "                    self.current_file_idx = 0\n",
    "                else:\n",
    "                    self.current_file_idx += 1\n",
    "                self.current_data = read_data_from_drive(self.file_paths[self.current_file_idx])\n",
    "                self.current_spectrogram = get_spectrogram_of_file(self.current_data)\n",
    "                self.current_pos_in_file = 0\n",
    "                self.current_pos_in_spectrogram = 0\n",
    "                break\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps, self.freq))\n",
    "        y = np.zeros((self.batch_size, self.num_classes))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_pos_in_spectrogram + self.num_steps >= len(self.current_spectrogram):\n",
    "                    self.__change_file()\n",
    "                x[i, :, :] = self.current_spectrogram[self.current_pos_in_spectrogram:self.current_pos_in_spectrogram + self.num_steps, :]\n",
    "                y[i, :] = to_categorical(self.file_classes[self.current_file_idx], num_classes=self.num_classes)\n",
    "                self.current_pos_in_spectrogram += self.skip_steps\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 60\n",
    "skip_steps = 30\n",
    "batch_size = 16\n",
    "num_classes = len(class_to_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = StepanBatchGenerator(train_data, num_steps, batch_size, num_classes, vocabulary, skip_steps=skip_steps)\n",
    "valid_data_generator = StepanBatchGenerator(valid_data, num_steps, batch_size, num_classes, vocabulary, skip_steps=skip_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pieces_in_whole_sound_with_step(data_count, num_steps, skip_steps):\n",
    "    pieces = 0\n",
    "    end_piece = num_steps\n",
    "    while end_piece <= data_count:\n",
    "        pieces += 1\n",
    "        end_piece += skip_steps\n",
    "    return pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = pieces_in_whole_sound_with_step(train_data_items_count, num_steps, skip_steps)//batch_size\n",
    "validation_steps = pieces_in_whole_sound_with_step(valid_data_items_count, num_steps, skip_steps)//batch_size\n",
    "print('Train steps: ', train_steps)\n",
    "print('Valid steps: ', validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, Dropout, TimeDistributed, LSTM\n",
    "from keras.initializers import Constant, RandomNormal\n",
    "\n",
    "hidden_size = 600\n",
    "num_epochs = 30\n",
    "\n",
    "stepan_model = Sequential()\n",
    "stepan_model.add(LSTM(hidden_size, dropout=0.0, return_sequences=False, input_shape=(num_steps, 129)))\n",
    "stepan_model.add(Dense(units=num_classes, activation='softmax'))\n",
    "                 \n",
    "stepan_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "print(stepan_model.summary())\n",
    "                 \n",
    "hist = stepan_model.fit_generator(train_data_generator.generate(), train_steps, num_epochs,\n",
    "                        validation_data=valid_data_generator.generate(),\n",
    "                        validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepan_model.save(\"stepan_model_5_classes_(noise,stepan,catalog,kuber,item)_acc_84.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "stepan_model = load_model(\"stepan_model_5_classes_(noise,stepan,catalog,kuber,item)_acc_84.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_files_dir='fake-sounds'\n",
    "fake_files_paths = get_sound_files_paths(fake_files_dir)\n",
    "fake_data = read_full_data_from_drive(fake_files_paths)\n",
    "fake_data = split_data_to_train_and_valid(fake_data, valid_percent=0.0)\n",
    "fake_data = fake_data[0]\n",
    "for path_to_file, cl in fake_data.items():\n",
    "    fake_data[path_to_file] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummy_iters = 0\n",
    "test_data = fake_data\n",
    "example_training_generator = StepanBatchGenerator(test_data, num_steps, 1, num_classes, vocabulary, skip_steps=skip_steps)\n",
    "print(\"Training data:\")\n",
    "#for i in range(dummy_iters):\n",
    "#    dummy = next(example_training_generator.generate())\n",
    "num_predict = 300\n",
    "for i in range(num_predict):\n",
    "    data = next(example_training_generator.generate())\n",
    "    prediction = stepan_model.predict_proba(data[0])\n",
    "    predicted = np.argmax(prediction)\n",
    "    true = np.argmax(data[1])\n",
    "    print('Label {}: predicted: {} true: {} p: {}'.format(i, predicted, true, prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
